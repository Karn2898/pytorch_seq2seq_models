{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QNvwqzPkDKt"
      },
      "source": [
        "#Text summarizer using seq2seq model in pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sJrnWKqjKGcu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NRdRllyhKfS8"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,input_size,emb_dim,enc_hid_dim , dec_hid_dim,dropout):\n",
        "        super().__init__()\n",
        "        self.embedding=nn.Embedding(input_size,emb_dim)\n",
        "        self.rnn=nn.GRU(emb_dim,enc_hid_dim,bidirectional=True)\n",
        "        self.fc=nn.Linear(enc_hid_dim*2 , dec_hid_dim)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,src):\n",
        "        embedded=self.dropout(self.embedding(src))\n",
        "\n",
        "        outputs,hidden=self.rnn(embedded)\n",
        "        hidden=torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]),\n",
        "                                          dim=1)))\n",
        "        return outputs,hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wLfYeYXHLvmr"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim,dec_hid_dim):\n",
        "        super().__init__()\n",
        "        self.attn=nn.Linear((enc_hid_dim*2)+dec_hid_dim,dec_hid_dim)\n",
        "        self.v=nn.Linear(dec_hid_dim,1,bias=False)\n",
        "\n",
        "    def forward (self, hidden,encoder_outputs):\n",
        "        batch_size=encoder_outputs.shape[1]\n",
        "        src_len=encoder_outputs.shape[0]\n",
        "        # Repeat decoder hidden state src_len times\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "        encoder_outputs=encoder_outputs.transpose(0, 1)\n",
        "\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        # energy: [batch_size, src_len, dec_hid_dim]\n",
        "\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        # attention: [batch_size, src_len]\n",
        "\n",
        "        return F.softmax(attention, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "IiJTvw7XOF8t"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + emb_dim + dec_hid_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "\n",
        "        #input = [batch size]\n",
        "        #hidden = [batch size, dec_hid_dim]\n",
        "        #encoder_outputs = [src_len, batch size, enc_hid_dim * 2]\n",
        "\n",
        "        batch_size = input.shape[0] # Get batch size\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "\n",
        "        #input = [1, batch size]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        #embedded = [1, batch size, emb_dim]\n",
        "\n",
        "        a = self.attention(hidden, encoder_outputs)\n",
        "\n",
        "        #a = [batch size, src_len]\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "        #encoder_outputs = [batch size, src_len, enc_hid_dim * 2]\n",
        "\n",
        "        weighted = torch.bmm(a.unsqueeze(1), encoder_outputs).squeeze(1)\n",
        "\n",
        "        #weighted = [batch size, enc_hid_dim * 2]\n",
        "\n",
        "        rnn_input = torch.cat((embedded, weighted.unsqueeze(0)), dim = 2)\n",
        "\n",
        "        #rnn_input = [1, batch size, (enc_hid_dim * 2) + emb_dim]\n",
        "\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "\n",
        "        #output = [seq_len, batch size, dec_hid_dim]\n",
        "        #hidden = [seq_len, batch size, dec_hid_dim]\n",
        "\n",
        "        #seq_len is 1 for the decoder\n",
        "\n",
        "        #output = [1, batch size, dec_hid_dim]\n",
        "        #hidden = [1, batch size, dec_hid_dim]\n",
        "\n",
        "        # Ensure tensors have 2 dimensions [batch_size, feature_dim] before concatenation for fc_out\n",
        "        embedded = embedded.squeeze(0).view(batch_size, -1) # Shape: [batch size, emb_dim]\n",
        "        output = output.squeeze(0).view(batch_size, -1)     # Shape: [batch size, dec_hid_dim]\n",
        "        weighted = weighted.squeeze(0).view(batch_size, -1) # Shape: [batch size, enc_hid_dim * 2]\n",
        "\n",
        "\n",
        "        # Print shapes for debugging\n",
        "        print(\"Shape of output before concat:\", output.shape)\n",
        "        print(\"Shape of weighted before concat:\", weighted.shape)\n",
        "        print(\"Shape of embedded before concat:\", embedded.shape)\n",
        "\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "\n",
        "        #prediction = [batch size, output_dim]\n",
        "\n",
        "        return prediction, hidden.squeeze(0), a # Removed .squeeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4W1frXdbQsiO"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self,encoder,decoder,device):\n",
        "        super().__init__()\n",
        "        self.encoder=encoder\n",
        "        self.decoder=decoder\n",
        "        self.device=device\n",
        "\n",
        "    def forward(self,src,trg,teacher_forcing_ratio=0.5):\n",
        "        # src shape: [src_len, batch_size]\n",
        "        # trg shape: [trg_len, batch_size]\n",
        "\n",
        "        batch_size=src.shape[1] # Get batch size from input tensor\n",
        "        trg_len=trg.shape[0]\n",
        "        trg_vocab_size=self.decoder.output_dim\n",
        "\n",
        "        outputs=torch.zeros(trg_len,batch_size,trg_vocab_size).to(self.device)\n",
        "\n",
        "        encoder_outputs,hidden=self.encoder(src)\n",
        "        # encoder_outputs shape: [src_len, batch_size, enc_hid_dim * 2]\n",
        "        # hidden shape: [batch_size, dec_hid_dim]\n",
        "\n",
        "        # First input to decoder is <sos> token\n",
        "        input = trg[0,:] # shape: [batch_size]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs)\n",
        "            # output shape: [batch_size, output_dim]\n",
        "            # hidden shape: [batch_size, dec_hid_dim]\n",
        "\n",
        "            outputs[t] = output # Assign output to the correct slice\n",
        "\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WKVNWeRfSkCU"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "39xRyAlVSkoW"
      },
      "outputs": [],
      "source": [
        "class vocabullary:\n",
        "    def __init__(self,freq_threshold=5):\n",
        "        self.itos = {0:\"<PAD>\", 1:\"<SOS>\", 2:\"<eos>\", 3:\"<unk>\"}\n",
        "        self.stoi = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    def build_vocabullary(self,sentence_list):\n",
        "        frequencies = Counter()\n",
        "        idx = 4\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer(sentence):\n",
        "                frequencies[word] += 1\n",
        "\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "n4pcgPjBWZOX"
      },
      "outputs": [],
      "source": [
        "class vocabullary:\n",
        "    def __init__(self,freq_threshold=5):\n",
        "        self.itos = {0:\"<PAD>\", 1:\"<SOS>\", 2:\"<eos>\", 3:\"<unk>\"}\n",
        "        self.stoi = {\"<pad>\": 0, \"<SOS>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    def build_vocabullary(self,sentence_list):\n",
        "        frequencies = Counter()\n",
        "        idx = 4\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer(sentence):\n",
        "                frequencies[word] += 1\n",
        "\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def tokenizer(self, text):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "        return text.split()\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        tokenized_text = self.tokenizer(text)\n",
        "        return [self.stoi[token] if token in self.stoi else self.stoi[\"<unk>\"]\n",
        "                for token in tokenized_text]\n",
        "\n",
        "    def save(self, filepath):\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump({'stoi': self.stoi, 'itos': self.itos, 'freq_threshold': self.freq_threshold}, f)\n",
        "\n",
        "    @staticmethod\n",
        "    def load(filepath):\n",
        "        with open(filepath, 'rb') as f:\n",
        "            state = pickle.load(f)\n",
        "            vocab = vocabullary(state['freq_threshold'])\n",
        "            vocab.stoi = state['stoi']\n",
        "            vocab.itos = state['itos']\n",
        "            return vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "d16pPyOaXvLJ"
      },
      "outputs": [],
      "source": [
        "class SummarizationDataset(Dataset):\n",
        "    def __init__(self, csv_file, text_column, summary_column,\n",
        "                 text_vocab=None, summary_vocab=None,\n",
        "                 max_text_len=400, max_summary_len=100,\n",
        "                 freq_threshold=5):\n",
        "\n",
        "\n",
        "        # Use header=None because the file doesn't have headers\n",
        "        self.df=pd.read_csv(csv_file, sep='\\t', header=None)\n",
        "        # The first column is the index, the second is column 0\n",
        "\n",
        "        self.df = self.df.rename(columns={0: 'summary'})\n",
        "        self.text_column=text_column\n",
        "        self.summary_column=summary_column\n",
        "        self.max_text_len=max_text_len\n",
        "        self.max_summary_len=max_summary_len\n",
        "\n",
        "\n",
        "        # Build or use provided vocabularies\n",
        "        if text_vocab is None:\n",
        "            self.text_vocab = vocabullary(freq_threshold)\n",
        "            # Accessing the index for text and converting to string\n",
        "            self.text_vocab.build_vocabullary(self.df.index.astype(str).tolist())\n",
        "        else:\n",
        "            self.text_vocab = text_vocab\n",
        "\n",
        "        if summary_vocab is None:\n",
        "            self.summary_vocab = vocabullary(freq_threshold)\n",
        "            # Accessing the renamed column 'summary' for summary and converting to string\n",
        "            self.summary_vocab.build_vocabullary(self.df[self.summary_column].astype(str).tolist())\n",
        "            # Explicitly add <SOS> and <eos> to summary_vocab if not present\n",
        "            if \"<SOS>\" not in self.summary_vocab.stoi:\n",
        "                sos_index = 1\n",
        "                self.summary_vocab.stoi[\"<SOS>\"] = sos_index\n",
        "                self.summary_vocab.itos[sos_index] = \"<SOS>\"\n",
        "            if \"<eos>\" not in self.summary_vocab.stoi:\n",
        "                eos_index = 2\n",
        "                self.summary_vocab.stoi[\"<eos>\"] = eos_index\n",
        "                self.summary_vocab.itos[eos_index] = \"<eos>\"\n",
        "        else:\n",
        "            self.summary_vocab = summary_vocab\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if \"<SOS>\" not in self.summary_vocab.stoi:\n",
        "            sos_index = 1\n",
        "            self.summary_vocab.stoi[\"<SOS>\"] = sos_index\n",
        "            self.summary_vocab.itos[sos_index] = \"<SOS>\"\n",
        "        if \"<eos>\" not in self.summary_vocab.stoi:\n",
        "            eos_index = 2\n",
        "            self.summary_vocab.stoi[\"<eos>\"] = eos_index\n",
        "            self.summary_vocab.itos[eos_index] = \"<eos>\"\n",
        "\n",
        "\n",
        "        # Accessing text from the index using iloc and converting to string\n",
        "        text=str(self.df.index[idx])\n",
        "\n",
        "        summary=str(self.df.iloc[idx][self.summary_column])\n",
        "\n",
        "\n",
        "        text_numericalized = self.text_vocab.numericalize(text)[:self.max_text_len]\n",
        "        summary_numericalized = self.summary_vocab.numericalize(summary)[:self.max_summary_len]\n",
        "\n",
        "\n",
        "        # Add <sos> and <eos> tokens to summary\n",
        "        summary_numericalized = [self.summary_vocab.stoi[\"<SOS>\"]] + \\\n",
        "                                 summary_numericalized + \\\n",
        "                                 [self.summary_vocab.stoi[\"<eos>\"]]\n",
        "\n",
        "        return torch.tensor(text_numericalized), torch.tensor(summary_numericalized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IB7aefpiaHsJ"
      },
      "outputs": [],
      "source": [
        "class MyCollate:\n",
        "    def __init__(self, pad_idx):\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        texts = [item[0] for item in batch]\n",
        "        summaries = [item[1] for item in batch]\n",
        "\n",
        "        # Pad sequences\n",
        "        texts_padded = pad_sequence(texts, batch_first=False, padding_value=self.pad_idx)\n",
        "        summaries_padded = pad_sequence(summaries, batch_first=False, padding_value=self.pad_idx)\n",
        "\n",
        "        return texts_padded, summaries_padded\n",
        "\n",
        "def get_loader(csv_file, text_column, summary_column,\n",
        "               batch_size=32, num_workers=4,\n",
        "               text_vocab=None, summary_vocab=None,\n",
        "               shuffle=True, pin_memory=True):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fXLvqPnnaXKe"
      },
      "outputs": [],
      "source": [
        "#create dataloader for training and validation\n",
        "dataset=SummarizationDataset(\n",
        "    csv_file='/content/eng-fra.txt',\n",
        "    text_column=0,\n",
        "    summary_column=1 # Use integer index for the second column (French translation as summary)\n",
        ")\n",
        "\n",
        "pad_idx=dataset.summary_vocab.stoi[\"<pad>\"]\n",
        "\n",
        "loader=DataLoader(\n",
        "    dataset=dataset,\n",
        "    batch_size=32,\n",
        "    num_workers=0, # Setting num_workers to 0 to avoid pickling issues\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "    collate_fn=MyCollate(pad_idx=pad_idx)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a79b9420",
        "outputId": "5ef341a9-1e44-4e8b-f7c3-b33ba849308e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.\tVa !\n",
            "\n",
            "Run!\tCours !\n",
            "\n",
            "Run!\tCourez !\n",
            "\n",
            "Wow!\tÇa alors !\n",
            "\n",
            "Fire!\tAu feu !\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with open('/content/eng-fra.txt', 'r', encoding='utf-8') as f:\n",
        "    for i in range(5): # Displaying the first 5 lines\n",
        "        print(f.readline())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7362e587",
        "outputId": "97fbf7f6-3d01-414a-8897-331f5da23033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully extracted 'data/eng-fra.txt' to '/content/eng-fra.txt'\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_file_path = '/content/data.zip'\n",
        "extracted_file_path = '/content/eng-fra.txt'\n",
        "file_to_extract = 'data/eng-fra.txt'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Check if the file to extract exists in the zip archive\n",
        "    if file_to_extract in zip_ref.namelist():\n",
        "        zip_ref.extract(file_to_extract, '/content/')\n",
        "\n",
        "        os.rename(os.path.join('/content/', file_to_extract), extracted_file_path)\n",
        "        print(f\"Successfully extracted '{file_to_extract}' to '{extracted_file_path}'\")\n",
        "    else:\n",
        "        print(f\"File '{file_to_extract}' not found in the zip archive.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km2cdc90e7q6"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_Rop3Joabu-G"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, iterator, optimizer, criterion, clip, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, (src, trg) in enumerate(iterator):\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg)\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "\n",
        "        trg = trg[1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            # The loss here is the average loss over the batch\n",
        "            print(f'  Batch {i}/{len(iterator)}, Loss: {loss.item():.4f}')\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nF-vkifgpwvp",
        "outputId": "5abf9d51-fd1c-44fe-c855-1909ed2c2c44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training data...\n",
            "Loading validation data...\n",
            "Input vocabulary size: 4\n",
            "Output vocabulary size: 8415\n",
            "The model has 23,676,639 trainable parameters\n",
            "  Batch 0/4246, Loss: 9.0379\n",
            "  Batch 100/4246, Loss: 5.8359\n",
            "  Batch 200/4246, Loss: 5.6561\n",
            "  Batch 300/4246, Loss: 5.5926\n",
            "  Batch 400/4246, Loss: 5.9086\n",
            "  Batch 500/4246, Loss: 5.2205\n",
            "  Batch 600/4246, Loss: 5.4735\n",
            "  Batch 700/4246, Loss: 5.6010\n",
            "  Batch 800/4246, Loss: 5.2774\n",
            "  Batch 900/4246, Loss: 5.4433\n",
            "  Batch 1000/4246, Loss: 5.2191\n",
            "  Batch 1100/4246, Loss: 5.4535\n",
            "  Batch 1200/4246, Loss: 4.9235\n",
            "  Batch 1300/4246, Loss: 4.9366\n",
            "  Batch 1400/4246, Loss: 5.4727\n",
            "  Batch 1500/4246, Loss: 4.8283\n",
            "  Batch 1600/4246, Loss: 4.8509\n",
            "  Batch 1700/4246, Loss: 4.9275\n",
            "  Batch 1800/4246, Loss: 5.2844\n",
            "  Batch 1900/4246, Loss: 4.9880\n",
            "  Batch 2000/4246, Loss: 5.2888\n",
            "  Batch 2100/4246, Loss: 5.0611\n",
            "  Batch 2200/4246, Loss: 5.0510\n",
            "  Batch 2300/4246, Loss: 5.0996\n",
            "  Batch 2400/4246, Loss: 5.3060\n",
            "  Batch 2500/4246, Loss: 4.6702\n",
            "  Batch 2600/4246, Loss: 5.0848\n",
            "  Batch 2700/4246, Loss: 5.2263\n",
            "  Batch 2800/4246, Loss: 4.8139\n",
            "  Batch 2900/4246, Loss: 4.8479\n",
            "  Batch 3000/4246, Loss: 5.6234\n",
            "  Batch 3100/4246, Loss: 5.3509\n",
            "  Batch 3200/4246, Loss: 4.6039\n",
            "  Batch 3300/4246, Loss: 5.0457\n",
            "  Batch 3400/4246, Loss: 4.7518\n",
            "  Batch 3500/4246, Loss: 4.5701\n",
            "  Batch 3600/4246, Loss: 4.9821\n",
            "  Batch 3700/4246, Loss: 5.2808\n",
            "  Batch 3800/4246, Loss: 4.4643\n",
            "  Batch 3900/4246, Loss: 5.2276\n",
            "  Batch 4000/4246, Loss: 5.2573\n",
            "  Batch 4100/4246, Loss: 4.7141\n",
            "  Batch 4200/4246, Loss: 4.7959\n",
            "  [Saved Best Model]\n",
            "Epoch: 01 | Time: 5m 57s\n",
            "\tTrain Loss: 5.145 | Train PPL: 171.507\n",
            "\t Val. Loss: 5.596 |  Val. PPL: 269.345\n",
            "  Batch 0/4246, Loss: 4.6786\n",
            "  Batch 100/4246, Loss: 5.1868\n",
            "  Batch 200/4246, Loss: 4.6156\n",
            "  Batch 300/4246, Loss: 5.3307\n",
            "  Batch 400/4246, Loss: 5.3719\n",
            "  Batch 500/4246, Loss: 5.0698\n",
            "  Batch 600/4246, Loss: 4.8813\n",
            "  Batch 700/4246, Loss: 4.6996\n",
            "  Batch 800/4246, Loss: 4.4000\n",
            "  Batch 900/4246, Loss: 4.2110\n",
            "  Batch 1000/4246, Loss: 4.8023\n",
            "  Batch 1100/4246, Loss: 4.5384\n",
            "  Batch 1200/4246, Loss: 4.5144\n",
            "  Batch 1300/4246, Loss: 5.0828\n",
            "  Batch 1400/4246, Loss: 4.9794\n",
            "  Batch 1500/4246, Loss: 4.6851\n",
            "  Batch 1600/4246, Loss: 5.0578\n",
            "  Batch 1700/4246, Loss: 4.6356\n",
            "  Batch 1800/4246, Loss: 4.9678\n",
            "  Batch 1900/4246, Loss: 4.9027\n",
            "  Batch 2000/4246, Loss: 4.9258\n",
            "  Batch 2100/4246, Loss: 4.8185\n",
            "  Batch 2200/4246, Loss: 5.2337\n",
            "  Batch 2300/4246, Loss: 4.9809\n",
            "  Batch 2400/4246, Loss: 4.4080\n",
            "  Batch 2500/4246, Loss: 4.8446\n",
            "  Batch 2600/4246, Loss: 4.0460\n",
            "  Batch 2700/4246, Loss: 4.8469\n",
            "  Batch 2800/4246, Loss: 4.3802\n",
            "  Batch 2900/4246, Loss: 4.9198\n",
            "  Batch 3000/4246, Loss: 5.0517\n",
            "  Batch 3100/4246, Loss: 4.5710\n",
            "  Batch 3200/4246, Loss: 4.6574\n",
            "  Batch 3300/4246, Loss: 4.3982\n",
            "  Batch 3400/4246, Loss: 4.3286\n",
            "  Batch 3500/4246, Loss: 4.3189\n",
            "  Batch 3600/4246, Loss: 4.4440\n",
            "  Batch 3700/4246, Loss: 4.3952\n",
            "  Batch 3800/4246, Loss: 4.9140\n",
            "  Batch 3900/4246, Loss: 5.1641\n",
            "  Batch 4000/4246, Loss: 4.9838\n",
            "  Batch 4100/4246, Loss: 5.2755\n",
            "  Batch 4200/4246, Loss: 4.6133\n",
            "Epoch: 02 | Time: 5m 50s\n",
            "\tTrain Loss: 4.769 | Train PPL: 117.821\n",
            "\t Val. Loss: 5.623 |  Val. PPL: 276.656\n",
            "  Batch 0/4246, Loss: 4.5368\n",
            "  Batch 100/4246, Loss: 4.4607\n",
            "  Batch 200/4246, Loss: 4.8702\n",
            "  Batch 300/4246, Loss: 4.7932\n",
            "  Batch 400/4246, Loss: 4.4781\n",
            "  Batch 500/4246, Loss: 4.8856\n",
            "  Batch 600/4246, Loss: 4.6334\n",
            "  Batch 700/4246, Loss: 5.2517\n",
            "  Batch 800/4246, Loss: 4.1510\n",
            "  Batch 900/4246, Loss: 5.2671\n",
            "  Batch 1000/4246, Loss: 3.8035\n",
            "  Batch 1100/4246, Loss: 4.6389\n",
            "  Batch 1200/4246, Loss: 4.1841\n",
            "  Batch 1300/4246, Loss: 5.0904\n",
            "  Batch 1400/4246, Loss: 4.8414\n",
            "  Batch 1500/4246, Loss: 4.3895\n",
            "  Batch 1600/4246, Loss: 4.0409\n",
            "  Batch 1700/4246, Loss: 5.1711\n",
            "  Batch 1800/4246, Loss: 4.2536\n",
            "  Batch 1900/4246, Loss: 4.1512\n",
            "  Batch 2000/4246, Loss: 4.9048\n",
            "  Batch 2100/4246, Loss: 5.0733\n",
            "  Batch 2200/4246, Loss: 4.8236\n",
            "  Batch 2300/4246, Loss: 4.2036\n",
            "  Batch 2400/4246, Loss: 4.9466\n",
            "  Batch 2500/4246, Loss: 4.7165\n",
            "  Batch 2600/4246, Loss: 5.0535\n",
            "  Batch 2700/4246, Loss: 4.4346\n",
            "  Batch 2800/4246, Loss: 4.8944\n",
            "  Batch 2900/4246, Loss: 4.6962\n",
            "  Batch 3000/4246, Loss: 3.9303\n",
            "  Batch 3100/4246, Loss: 5.0115\n",
            "  Batch 3200/4246, Loss: 4.4626\n",
            "  Batch 3300/4246, Loss: 4.7715\n",
            "  Batch 3400/4246, Loss: 4.3166\n",
            "  Batch 3500/4246, Loss: 4.2408\n",
            "  Batch 3600/4246, Loss: 5.0809\n",
            "  Batch 3700/4246, Loss: 4.3421\n",
            "  Batch 3800/4246, Loss: 4.5472\n",
            "  Batch 3900/4246, Loss: 4.5709\n",
            "  Batch 4000/4246, Loss: 4.4458\n",
            "  Batch 4100/4246, Loss: 4.2454\n",
            "  Batch 4200/4246, Loss: 5.3252\n",
            "  [Saved Best Model]\n",
            "Epoch: 03 | Time: 5m 48s\n",
            "\tTrain Loss: 4.649 | Train PPL: 104.489\n",
            "\t Val. Loss: 5.594 |  Val. PPL: 268.736\n",
            "  Batch 0/4246, Loss: 4.2397\n",
            "  Batch 100/4246, Loss: 5.1332\n",
            "  Batch 200/4246, Loss: 5.5842\n",
            "  Batch 300/4246, Loss: 5.0605\n",
            "  Batch 400/4246, Loss: 5.2488\n",
            "  Batch 500/4246, Loss: 4.9925\n",
            "  Batch 600/4246, Loss: 4.5704\n",
            "  Batch 700/4246, Loss: 4.3554\n",
            "  Batch 800/4246, Loss: 5.2418\n",
            "  Batch 900/4246, Loss: 4.8799\n",
            "  Batch 1000/4246, Loss: 4.4303\n",
            "  Batch 1100/4246, Loss: 4.5323\n",
            "  Batch 1200/4246, Loss: 4.2088\n",
            "  Batch 1300/4246, Loss: 5.4076\n",
            "  Batch 1400/4246, Loss: 5.2520\n",
            "  Batch 1500/4246, Loss: 4.9439\n",
            "  Batch 1600/4246, Loss: 4.6544\n",
            "  Batch 1700/4246, Loss: 4.3531\n",
            "  Batch 1800/4246, Loss: 4.3511\n",
            "  Batch 1900/4246, Loss: 5.0708\n",
            "  Batch 2000/4246, Loss: 4.8517\n",
            "  Batch 2100/4246, Loss: 4.4139\n",
            "  Batch 2200/4246, Loss: 4.7065\n",
            "  Batch 2300/4246, Loss: 4.8321\n",
            "  Batch 2400/4246, Loss: 4.4978\n",
            "  Batch 2500/4246, Loss: 4.4899\n",
            "  Batch 2600/4246, Loss: 3.8706\n",
            "  Batch 2700/4246, Loss: 4.2441\n",
            "  Batch 2800/4246, Loss: 4.6060\n",
            "  Batch 2900/4246, Loss: 4.1671\n",
            "  Batch 3000/4246, Loss: 4.8928\n",
            "  Batch 3100/4246, Loss: 5.3881\n",
            "  Batch 3200/4246, Loss: 4.5904\n",
            "  Batch 3300/4246, Loss: 4.2944\n",
            "  Batch 3400/4246, Loss: 3.9915\n",
            "  Batch 3500/4246, Loss: 4.2930\n",
            "  Batch 3600/4246, Loss: 4.6001\n",
            "  Batch 3700/4246, Loss: 5.0307\n",
            "  Batch 3800/4246, Loss: 4.4178\n",
            "  Batch 3900/4246, Loss: 4.3844\n",
            "  Batch 4000/4246, Loss: 5.1135\n",
            "  Batch 4100/4246, Loss: 4.9173\n",
            "  Batch 4200/4246, Loss: 4.3052\n",
            "Epoch: 04 | Time: 5m 48s\n",
            "\tTrain Loss: 4.576 | Train PPL:  97.145\n",
            "\t Val. Loss: 5.614 |  Val. PPL: 274.222\n",
            "  Batch 0/4246, Loss: 4.2071\n",
            "  Batch 100/4246, Loss: 4.6006\n",
            "  Batch 200/4246, Loss: 4.3910\n",
            "  Batch 300/4246, Loss: 4.8841\n",
            "  Batch 400/4246, Loss: 4.5512\n",
            "  Batch 500/4246, Loss: 4.1833\n",
            "  Batch 600/4246, Loss: 4.2910\n",
            "  Batch 700/4246, Loss: 4.7082\n",
            "  Batch 800/4246, Loss: 4.2990\n",
            "  Batch 900/4246, Loss: 5.0678\n",
            "  Batch 1000/4246, Loss: 5.2639\n",
            "  Batch 1100/4246, Loss: 4.7189\n",
            "  Batch 1200/4246, Loss: 4.9696\n",
            "  Batch 1300/4246, Loss: 3.9856\n",
            "  Batch 1400/4246, Loss: 4.3242\n",
            "  Batch 1500/4246, Loss: 4.0940\n",
            "  Batch 1600/4246, Loss: 4.0757\n",
            "  Batch 1700/4246, Loss: 4.0266\n",
            "  Batch 1800/4246, Loss: 5.0713\n",
            "  Batch 1900/4246, Loss: 4.2038\n",
            "  Batch 2000/4246, Loss: 4.5344\n",
            "  Batch 2100/4246, Loss: 3.7681\n",
            "  Batch 2200/4246, Loss: 4.1825\n",
            "  Batch 2300/4246, Loss: 4.5484\n",
            "  Batch 2400/4246, Loss: 4.4075\n",
            "  Batch 2500/4246, Loss: 4.6138\n",
            "  Batch 2600/4246, Loss: 4.1423\n",
            "  Batch 2700/4246, Loss: 5.0263\n",
            "  Batch 2800/4246, Loss: 4.9646\n",
            "  Batch 2900/4246, Loss: 4.3515\n",
            "  Batch 3000/4246, Loss: 4.9677\n",
            "  Batch 3100/4246, Loss: 4.2294\n",
            "  Batch 3200/4246, Loss: 4.3575\n",
            "  Batch 3300/4246, Loss: 4.3046\n",
            "  Batch 3400/4246, Loss: 4.6034\n",
            "  Batch 3500/4246, Loss: 5.3914\n",
            "  Batch 3600/4246, Loss: 4.3790\n",
            "  Batch 3700/4246, Loss: 4.4105\n",
            "  Batch 3800/4246, Loss: 5.3653\n",
            "  Batch 3900/4246, Loss: 4.3597\n",
            "  Batch 4000/4246, Loss: 4.1475\n",
            "  Batch 4100/4246, Loss: 4.4693\n",
            "  Batch 4200/4246, Loss: 3.9977\n",
            "Epoch: 05 | Time: 5m 48s\n",
            "\tTrain Loss: 4.516 | Train PPL:  91.456\n",
            "\t Val. Loss: 5.598 |  Val. PPL: 269.892\n",
            "  Batch 0/4246, Loss: 4.9615\n",
            "  Batch 100/4246, Loss: 4.5407\n",
            "  Batch 200/4246, Loss: 4.8546\n",
            "  Batch 300/4246, Loss: 4.7888\n",
            "  Batch 400/4246, Loss: 4.7399\n",
            "  Batch 500/4246, Loss: 4.4165\n",
            "  Batch 600/4246, Loss: 4.5701\n",
            "  Batch 700/4246, Loss: 4.6118\n",
            "  Batch 800/4246, Loss: 4.3335\n",
            "  Batch 900/4246, Loss: 3.5618\n",
            "  Batch 1000/4246, Loss: 4.5577\n",
            "  Batch 1100/4246, Loss: 4.8084\n",
            "  Batch 1200/4246, Loss: 4.6464\n",
            "  Batch 1300/4246, Loss: 4.0050\n",
            "  Batch 1400/4246, Loss: 4.1161\n",
            "  Batch 1500/4246, Loss: 4.4091\n",
            "  Batch 1600/4246, Loss: 3.2921\n",
            "  Batch 1700/4246, Loss: 4.1862\n",
            "  Batch 1800/4246, Loss: 3.8229\n",
            "  Batch 1900/4246, Loss: 4.5668\n",
            "  Batch 2000/4246, Loss: 4.6822\n",
            "  Batch 2100/4246, Loss: 5.6736\n",
            "  Batch 2200/4246, Loss: 4.6582\n",
            "  Batch 2300/4246, Loss: 5.4745\n",
            "  Batch 2400/4246, Loss: 3.8685\n",
            "  Batch 2500/4246, Loss: 3.8561\n",
            "  Batch 2600/4246, Loss: 3.8532\n",
            "  Batch 2700/4246, Loss: 3.9788\n",
            "  Batch 2800/4246, Loss: 5.3140\n",
            "  Batch 2900/4246, Loss: 4.1029\n",
            "  Batch 3000/4246, Loss: 4.1408\n",
            "  Batch 3100/4246, Loss: 4.5178\n",
            "  Batch 3200/4246, Loss: 4.1134\n",
            "  Batch 3300/4246, Loss: 4.3562\n",
            "  Batch 3400/4246, Loss: 4.4347\n",
            "  Batch 3500/4246, Loss: 5.0229\n",
            "  Batch 3600/4246, Loss: 4.9804\n",
            "  Batch 3700/4246, Loss: 5.1611\n",
            "  Batch 3800/4246, Loss: 5.4172\n",
            "  Batch 3900/4246, Loss: 4.1864\n",
            "  Batch 4000/4246, Loss: 3.8546\n",
            "  Batch 4100/4246, Loss: 5.2251\n",
            "  Batch 4200/4246, Loss: 4.2474\n",
            "  [Saved Best Model]\n",
            "Epoch: 06 | Time: 5m 47s\n",
            "\tTrain Loss: 4.483 | Train PPL:  88.488\n",
            "\t Val. Loss: 5.573 |  Val. PPL: 263.240\n",
            "  Batch 0/4246, Loss: 4.1018\n",
            "  Batch 100/4246, Loss: 4.6925\n",
            "  Batch 200/4246, Loss: 5.1495\n",
            "  Batch 300/4246, Loss: 4.7450\n",
            "  Batch 400/4246, Loss: 3.7122\n",
            "  Batch 500/4246, Loss: 4.3736\n",
            "  Batch 600/4246, Loss: 4.3669\n",
            "  Batch 700/4246, Loss: 3.6873\n",
            "  Batch 800/4246, Loss: 3.9791\n",
            "  Batch 900/4246, Loss: 3.6994\n",
            "  Batch 1000/4246, Loss: 4.0499\n",
            "  Batch 1100/4246, Loss: 4.1743\n",
            "  Batch 1200/4246, Loss: 4.1792\n",
            "  Batch 1300/4246, Loss: 4.6701\n",
            "  Batch 1400/4246, Loss: 4.5924\n",
            "  Batch 1500/4246, Loss: 4.2064\n",
            "  Batch 1600/4246, Loss: 5.2278\n",
            "  Batch 1700/4246, Loss: 3.8486\n",
            "  Batch 1800/4246, Loss: 4.3958\n",
            "  Batch 1900/4246, Loss: 5.0453\n",
            "  Batch 2000/4246, Loss: 4.7350\n",
            "  Batch 2100/4246, Loss: 4.3736\n",
            "  Batch 2200/4246, Loss: 5.4191\n",
            "  Batch 2300/4246, Loss: 3.8421\n",
            "  Batch 2400/4246, Loss: 4.9337\n",
            "  Batch 2500/4246, Loss: 5.1813\n",
            "  Batch 2600/4246, Loss: 4.4152\n",
            "  Batch 2700/4246, Loss: 4.8476\n",
            "  Batch 2800/4246, Loss: 5.4443\n",
            "  Batch 2900/4246, Loss: 4.1812\n",
            "  Batch 3000/4246, Loss: 3.9428\n",
            "  Batch 3100/4246, Loss: 4.1616\n",
            "  Batch 3200/4246, Loss: 5.0373\n",
            "  Batch 3300/4246, Loss: 4.5266\n",
            "  Batch 3400/4246, Loss: 3.9307\n",
            "  Batch 3500/4246, Loss: 4.4266\n",
            "  Batch 3600/4246, Loss: 4.7809\n",
            "  Batch 3700/4246, Loss: 3.7944\n",
            "  Batch 3800/4246, Loss: 4.1186\n",
            "  Batch 3900/4246, Loss: 4.0054\n",
            "  Batch 4000/4246, Loss: 4.5813\n",
            "  Batch 4100/4246, Loss: 4.5216\n",
            "  Batch 4200/4246, Loss: 4.9477\n",
            "Epoch: 07 | Time: 5m 47s\n",
            "\tTrain Loss: 4.456 | Train PPL:  86.164\n",
            "\t Val. Loss: 5.593 |  Val. PPL: 268.441\n",
            "  Batch 0/4246, Loss: 5.3926\n",
            "  Batch 100/4246, Loss: 4.3963\n",
            "  Batch 200/4246, Loss: 4.3168\n",
            "  Batch 300/4246, Loss: 5.0173\n",
            "  Batch 400/4246, Loss: 4.9295\n",
            "  Batch 500/4246, Loss: 4.5326\n",
            "  Batch 600/4246, Loss: 4.3275\n",
            "  Batch 700/4246, Loss: 4.3954\n",
            "  Batch 800/4246, Loss: 4.5017\n",
            "  Batch 900/4246, Loss: 3.6897\n",
            "  Batch 1000/4246, Loss: 4.0816\n",
            "  Batch 1100/4246, Loss: 3.9806\n",
            "  Batch 1200/4246, Loss: 3.9482\n",
            "  Batch 1300/4246, Loss: 4.7756\n",
            "  Batch 1400/4246, Loss: 4.7852\n",
            "  Batch 1500/4246, Loss: 4.4745\n",
            "  Batch 1600/4246, Loss: 4.5777\n",
            "  Batch 1700/4246, Loss: 4.2298\n",
            "  Batch 1800/4246, Loss: 4.9948\n",
            "  Batch 1900/4246, Loss: 4.8045\n",
            "  Batch 2000/4246, Loss: 3.7335\n",
            "  Batch 2100/4246, Loss: 4.3227\n",
            "  Batch 2200/4246, Loss: 4.4643\n",
            "  Batch 2300/4246, Loss: 3.7437\n",
            "  Batch 2400/4246, Loss: 3.6359\n",
            "  Batch 2500/4246, Loss: 4.6359\n",
            "  Batch 2600/4246, Loss: 4.8267\n",
            "  Batch 2700/4246, Loss: 4.3965\n",
            "  Batch 2800/4246, Loss: 4.2182\n",
            "  Batch 2900/4246, Loss: 4.2355\n",
            "  Batch 3000/4246, Loss: 4.0225\n",
            "  Batch 3100/4246, Loss: 4.7657\n",
            "  Batch 3200/4246, Loss: 4.6293\n",
            "  Batch 3300/4246, Loss: 4.4350\n",
            "  Batch 3400/4246, Loss: 4.0169\n",
            "  Batch 3500/4246, Loss: 4.8021\n",
            "  Batch 3600/4246, Loss: 5.2221\n",
            "  Batch 3700/4246, Loss: 4.4338\n",
            "  Batch 3800/4246, Loss: 5.6105\n",
            "  Batch 3900/4246, Loss: 5.1913\n",
            "  Batch 4000/4246, Loss: 4.3648\n",
            "  Batch 4100/4246, Loss: 4.7244\n",
            "  Batch 4200/4246, Loss: 4.4718\n",
            "Epoch: 08 | Time: 5m 47s\n",
            "\tTrain Loss: 4.436 | Train PPL:  84.417\n",
            "\t Val. Loss: 5.600 |  Val. PPL: 270.414\n",
            "  Batch 0/4246, Loss: 4.5074\n",
            "  Batch 100/4246, Loss: 3.7139\n",
            "  Batch 200/4246, Loss: 4.4560\n",
            "  Batch 300/4246, Loss: 5.4217\n",
            "  Batch 400/4246, Loss: 4.5169\n",
            "  Batch 500/4246, Loss: 4.5560\n",
            "  Batch 600/4246, Loss: 4.5065\n",
            "  Batch 700/4246, Loss: 4.3898\n",
            "  Batch 800/4246, Loss: 5.1926\n",
            "  Batch 900/4246, Loss: 3.9331\n",
            "  Batch 1000/4246, Loss: 4.5506\n",
            "  Batch 1100/4246, Loss: 4.3398\n",
            "  Batch 1200/4246, Loss: 4.2403\n",
            "  Batch 1300/4246, Loss: 4.3222\n",
            "  Batch 1400/4246, Loss: 4.7468\n",
            "  Batch 1500/4246, Loss: 4.5281\n",
            "  Batch 1600/4246, Loss: 4.4622\n",
            "  Batch 1700/4246, Loss: 4.8237\n",
            "  Batch 1800/4246, Loss: 3.7752\n",
            "  Batch 1900/4246, Loss: 4.0179\n",
            "  Batch 2000/4246, Loss: 4.0518\n",
            "  Batch 2100/4246, Loss: 3.8625\n",
            "  Batch 2200/4246, Loss: 4.3379\n",
            "  Batch 2300/4246, Loss: 4.9593\n",
            "  Batch 2400/4246, Loss: 4.1857\n",
            "  Batch 2500/4246, Loss: 4.7059\n",
            "  Batch 2600/4246, Loss: 4.0172\n",
            "  Batch 2700/4246, Loss: 3.8293\n",
            "  Batch 2800/4246, Loss: 4.1950\n",
            "  Batch 2900/4246, Loss: 3.8797\n",
            "  Batch 3000/4246, Loss: 4.7196\n",
            "  Batch 3100/4246, Loss: 4.5819\n",
            "  Batch 3200/4246, Loss: 4.0961\n",
            "  Batch 3300/4246, Loss: 4.6415\n",
            "  Batch 3400/4246, Loss: 4.3633\n",
            "  Batch 3500/4246, Loss: 5.0058\n",
            "  Batch 3600/4246, Loss: 4.6420\n",
            "  Batch 3700/4246, Loss: 4.0700\n",
            "  Batch 3800/4246, Loss: 5.3174\n",
            "  Batch 3900/4246, Loss: 4.1099\n",
            "  Batch 4000/4246, Loss: 4.5537\n",
            "  Batch 4100/4246, Loss: 4.2299\n",
            "  Batch 4200/4246, Loss: 3.5511\n",
            "Epoch: 09 | Time: 5m 47s\n",
            "\tTrain Loss: 4.408 | Train PPL:  82.104\n",
            "\t Val. Loss: 5.592 |  Val. PPL: 268.143\n",
            "  Batch 0/4246, Loss: 4.8887\n",
            "  Batch 100/4246, Loss: 3.8170\n",
            "  Batch 200/4246, Loss: 4.3394\n",
            "  Batch 300/4246, Loss: 4.2523\n",
            "  Batch 400/4246, Loss: 4.2582\n",
            "  Batch 500/4246, Loss: 4.1416\n",
            "  Batch 600/4246, Loss: 4.1310\n",
            "  Batch 700/4246, Loss: 4.2961\n",
            "  Batch 800/4246, Loss: 4.7393\n",
            "  Batch 900/4246, Loss: 3.7178\n",
            "  Batch 1000/4246, Loss: 4.1169\n",
            "  Batch 1100/4246, Loss: 5.1640\n",
            "  Batch 1200/4246, Loss: 4.2556\n",
            "  Batch 1300/4246, Loss: 5.0707\n",
            "  Batch 1400/4246, Loss: 5.0110\n",
            "  Batch 1500/4246, Loss: 3.7327\n",
            "  Batch 1600/4246, Loss: 4.6469\n",
            "  Batch 1700/4246, Loss: 4.7465\n",
            "  Batch 1800/4246, Loss: 4.4322\n",
            "  Batch 1900/4246, Loss: 4.6284\n",
            "  Batch 2000/4246, Loss: 4.4089\n",
            "  Batch 2100/4246, Loss: 4.1159\n",
            "  Batch 2200/4246, Loss: 3.7950\n",
            "  Batch 2300/4246, Loss: 4.4655\n",
            "  Batch 2400/4246, Loss: 4.0284\n",
            "  Batch 2500/4246, Loss: 4.5379\n",
            "  Batch 2600/4246, Loss: 3.9320\n",
            "  Batch 2700/4246, Loss: 4.3285\n",
            "  Batch 2800/4246, Loss: 4.1025\n",
            "  Batch 2900/4246, Loss: 4.1527\n",
            "  Batch 3000/4246, Loss: 4.0621\n",
            "  Batch 3100/4246, Loss: 4.7160\n",
            "  Batch 3200/4246, Loss: 3.8674\n",
            "  Batch 3300/4246, Loss: 3.8238\n",
            "  Batch 3400/4246, Loss: 3.4770\n",
            "  Batch 3500/4246, Loss: 3.8578\n",
            "  Batch 3600/4246, Loss: 4.2838\n",
            "  Batch 3700/4246, Loss: 4.6406\n",
            "  Batch 3800/4246, Loss: 3.7408\n",
            "  Batch 3900/4246, Loss: 3.6845\n",
            "  Batch 4000/4246, Loss: 4.3992\n",
            "  Batch 4100/4246, Loss: 4.5228\n",
            "  Batch 4200/4246, Loss: 3.4384\n",
            "Epoch: 10 | Time: 5m 47s\n",
            "\tTrain Loss: 4.323 | Train PPL:  75.428\n",
            "\t Val. Loss: 5.586 |  Val. PPL: 266.619\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def train_epoch(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, (src, trg) in enumerate(iterator):\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg)\n",
        "\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1).to(torch.long)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f'  Batch {i}/{len(iterator)}, Loss: {loss.item():.4f}')\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (src, trg) in enumerate(iterator):\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "            # Turn off teacher forcing\n",
        "            output = model(src, trg, teacher_forcing_ratio=0)\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1).to(torch.long)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "# Configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "INPUT_DIM = 10000\n",
        "OUTPUT_DIM = 10000\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "BATCH_SIZE = 64\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "\n",
        "print(\"Loading training data...\")\n",
        "\n",
        "train_loader = loader\n",
        "train_dataset = dataset\n",
        "val_loader = loader\n",
        "print(\"Loading validation data...\")\n",
        "\n",
        "val_dataset = dataset\n",
        "\n",
        "\n",
        "# Update dimensions based on actual vocabulary size\n",
        "INPUT_DIM = len(train_dataset.text_vocab)\n",
        "OUTPUT_DIM = len(train_dataset.summary_vocab)\n",
        "\n",
        "print(f\"Input vocabulary size: {INPUT_DIM}\")\n",
        "print(f\"Output vocabulary size: {OUTPUT_DIM}\")\n",
        "\n",
        "# Save vocabularies\n",
        "train_dataset.text_vocab.save('text_vocab.pkl')\n",
        "train_dataset.summary_vocab.save('summary_vocab.pkl')\n",
        "\n",
        "# Initialize model\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "# Initialize weights\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "\n",
        "model.apply(init_weights)\n",
        "\n",
        "# Count parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "# Optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "PAD_IDX = train_dataset.summary_vocab.stoi['<pad>']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "# Training loop\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, val_loader, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(valid_loss)\n",
        "\n",
        "    # Save best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'seq2seqsummarizer_model.pt')\n",
        "        print('  [Saved Best Model]')\n",
        "\n",
        "    # Save checkpoint\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'train_loss': train_loss,\n",
        "        'valid_loss': valid_loss,\n",
        "    }\n",
        "    torch.save(checkpoint, f'checkpoint_epoch_{epoch}.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhaXD733wJUP"
      },
      "source": [
        "#Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "lfIChki0uyJo"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(model, sentence, text_vocab, summary_vocab, device, max_length=100):\n",
        "\n",
        "    model.eval()\n",
        "    # Tokenize and numericalize\n",
        "    tokens = text_vocab.numericalize(sentence)\n",
        "    tokens = torch.LongTensor(tokens).unsqueeze(1).to(device)\n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = model.encoder(tokens)\n",
        "\n",
        "\n",
        "    # Start with <sos> token\n",
        "    trg_indexes = [summary_vocab.stoi['<SOS>']]\n",
        "    attentions = []\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs)\n",
        "\n",
        "        attentions.append(attention)\n",
        "        pred_token = output.argmax(1).item()\n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == summary_vocab.stoi['<eos>']:\n",
        "            break\n",
        "\n",
        "    # Convert indexes to words\n",
        "    trg_tokens = [summary_vocab.itos[i] for i in trg_indexes]\n",
        "\n",
        "    # Remove <sos> and <eos>\n",
        "    return trg_tokens[1:-1] if trg_tokens[-1] == '<eos>' else trg_tokens[1:], attentions\n",
        "\n",
        "def summarize_text(text, model, text_vocab, summary_vocab, device):\n",
        "\n",
        "    summary_tokens, attentions = translate_sentence(\n",
        "        model, text, text_vocab, summary_vocab, device\n",
        "    )\n",
        "    summary = ' '.join(summary_tokens)\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "q3x8D0Avvgdq"
      },
      "outputs": [],
      "source": [
        "def load_model(model_path, text_vocab_path, summary_vocab_path, device):\n",
        "    \"\"\"\n",
        "    Load trained model and vocabularies\n",
        "    \"\"\"\n",
        "    # Load vocabularies\n",
        "    text_vocab = vocabullary.load(text_vocab_path)\n",
        "    summary_vocab = vocabullary.load(summary_vocab_path)\n",
        "\n",
        "    # Model configuration (must match training)\n",
        "    INPUT_DIM = len(text_vocab)\n",
        "    OUTPUT_DIM = len(summary_vocab)\n",
        "    ENC_EMB_DIM = 256\n",
        "    DEC_EMB_DIM = 256\n",
        "    ENC_HID_DIM = 512\n",
        "    DEC_HID_DIM = 512\n",
        "    ENC_DROPOUT = 0.5\n",
        "    DEC_DROPOUT = 0.5\n",
        "\n",
        "    # Initialize model\n",
        "    attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "    model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "    # Load weights\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    return model, text_vocab, summary_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Mg-YKRcOvmgp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07d0fc32-4e69-47fc-a9fa-5b86822373f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of output before concat: torch.Size([1, 512])\n",
            "Shape of weighted before concat: torch.Size([1, 1024])\n",
            "Shape of embedded before concat: torch.Size([1, 256])\n",
            "Shape of output before concat: torch.Size([1, 512])\n",
            "Shape of weighted before concat: torch.Size([1, 1024])\n",
            "Shape of embedded before concat: torch.Size([1, 256])\n",
            "Shape of output before concat: torch.Size([1, 512])\n",
            "Shape of weighted before concat: torch.Size([1, 1024])\n",
            "Shape of embedded before concat: torch.Size([1, 256])\n",
            "Shape of output before concat: torch.Size([1, 512])\n",
            "Shape of weighted before concat: torch.Size([1, 1024])\n",
            "Shape of embedded before concat: torch.Size([1, 256])\n",
            "Original Text:\n",
            "\n",
            "Artificial intelligence is transforming the world in numerous ways.\n",
            "Machine learning algorithms are now being used in healthcare to diagnose diseases,\n",
            "in finance to detect fraud, and in transportation to power self-driving cars.\n",
            "The technology continues to advance rapidly with new breakthroughs happening regularly.\n",
            "\n",
            "\n",
            "Generated Summary:\n",
            "je ne <unk>\n",
            "\n",
            "\n",
            "New Text Example:\n",
            "\n",
            "The quick brown fox jumps over the lazy dog. This sentence is often used to test\n",
            "typewriters or keyboards because it contains all the letters of the alphabet.\n",
            "It's a classic pangram.\n",
            "\n",
            "\n",
            "Generated Summary for New Text:\n",
            "Shape of output before concat: torch.Size([1, 512])\n",
            "Shape of weighted before concat: torch.Size([1, 1024])\n",
            "Shape of embedded before concat: torch.Size([1, 256])\n",
            "Shape of output before concat: torch.Size([1, 512])\n",
            "Shape of weighted before concat: torch.Size([1, 1024])\n",
            "Shape of embedded before concat: torch.Size([1, 256])\n",
            "Shape of output before concat: torch.Size([1, 512])\n",
            "Shape of weighted before concat: torch.Size([1, 1024])\n",
            "Shape of embedded before concat: torch.Size([1, 256])\n",
            "Shape of output before concat: torch.Size([1, 512])\n",
            "Shape of weighted before concat: torch.Size([1, 1024])\n",
            "Shape of embedded before concat: torch.Size([1, 256])\n",
            "je ne <unk>\n",
            "\n",
            "\n",
            "Batch Summarization:\n",
            "Shape of output before concat: torch.Size([1, 512])\n",
            "Shape of weighted before concat: torch.Size([1, 1024])\n",
            "Shape of embedded before concat: torch.Size([1, 256])\n",
            "Shape of output before concat: torch.Size([1, 512])\n",
            "Shape of weighted before concat: torch.Size([1, 1024])\n",
            "Shape of embedded before concat: torch.Size([1, 256])\n",
            "Shape of output before concat: torch.Size([1, 512])\n",
            "Shape of weighted before concat: torch.Size([1, 1024])\n",
            "Shape of embedded before concat: torch.Size([1, 256])\n",
            "Shape of output before concat: torch.Size([1, 512])\n",
            "Shape of weighted before concat: torch.Size([1, 1024])\n",
            "Shape of embedded before concat: torch.Size([1, 256])\n",
            "\n",
            "1. Summary: je ne <unk>\n",
            "Shape of output before concat: torch.Size([1, 512])\n",
            "Shape of weighted before concat: torch.Size([1, 1024])\n",
            "Shape of embedded before concat: torch.Size([1, 256])\n",
            "Shape of output before concat: torch.Size([1, 512])\n",
            "Shape of weighted before concat: torch.Size([1, 1024])\n",
            "Shape of embedded before concat: torch.Size([1, 256])\n",
            "Shape of output before concat: torch.Size([1, 512])\n",
            "Shape of weighted before concat: torch.Size([1, 1024])\n",
            "Shape of embedded before concat: torch.Size([1, 256])\n",
            "Shape of output before concat: torch.Size([1, 512])\n",
            "Shape of weighted before concat: torch.Size([1, 1024])\n",
            "Shape of embedded before concat: torch.Size([1, 256])\n",
            "\n",
            "2. Summary: je ne <unk>\n",
            "Shape of output before concat: torch.Size([1, 512])\n",
            "Shape of weighted before concat: torch.Size([1, 1024])\n",
            "Shape of embedded before concat: torch.Size([1, 256])\n",
            "Shape of output before concat: torch.Size([1, 512])\n",
            "Shape of weighted before concat: torch.Size([1, 1024])\n",
            "Shape of embedded before concat: torch.Size([1, 256])\n",
            "Shape of output before concat: torch.Size([1, 512])\n",
            "Shape of weighted before concat: torch.Size([1, 1024])\n",
            "Shape of embedded before concat: torch.Size([1, 256])\n",
            "Shape of output before concat: torch.Size([1, 512])\n",
            "Shape of weighted before concat: torch.Size([1, 1024])\n",
            "Shape of embedded before concat: torch.Size([1, 256])\n",
            "\n",
            "3. Summary: je ne <unk>\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Load model\n",
        "    model, text_vocab, summary_vocab = load_model(\n",
        "        model_path='seq2seqsummarizer_model.pt',\n",
        "        text_vocab_path='text_vocab.pkl',\n",
        "        summary_vocab_path='summary_vocab.pkl',\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Example text\n",
        "    text = \"\"\"\n",
        "Artificial intelligence is transforming the world in numerous ways.\n",
        "Machine learning algorithms are now being used in healthcare to diagnose diseases,\n",
        "in finance to detect fraud, and in transportation to power self-driving cars.\n",
        "The technology continues to advance rapidly with new breakthroughs happening regularly.\n",
        "\"\"\"\n",
        "\n",
        "    # Generate summary\n",
        "    summary = summarize_text(text, model, text_vocab, summary_vocab, device)\n",
        "\n",
        "    print(\"Original Text:\")\n",
        "    print(text)\n",
        "    print(\"\\nGenerated Summary:\")\n",
        "    print(summary)\n",
        "\n",
        "    # Add a new text example\n",
        "    new_text = \"\"\"\n",
        "The quick brown fox jumps over the lazy dog. This sentence is often used to test\n",
        "typewriters or keyboards because it contains all the letters of the alphabet.\n",
        "It's a classic pangram.\n",
        "\"\"\"\n",
        "\n",
        "    print(\"\\n\\nNew Text Example:\")\n",
        "    print(new_text)\n",
        "    print(\"\\nGenerated Summary for New Text:\")\n",
        "    new_summary = summarize_text(new_text, model, text_vocab, summary_vocab, device)\n",
        "    print(new_summary)\n",
        "\n",
        "    # Batch inference example\n",
        "    texts = [\n",
        "        \"Your first document here...\",\n",
        "        \"Your second document here...\",\n",
        "        \"Your third document here...\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\n\\nBatch Summarization:\")\n",
        "    for i, text in enumerate(texts, 1):\n",
        "        summary = summarize_text(text, model, text_vocab, summary_vocab, device)\n",
        "        print(f\"\\n{i}. Summary: {summary}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aj3uFJvEriGH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}